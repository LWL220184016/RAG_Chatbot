from langchain.callbacks.base import BaseCallbackHandler
from collections import deque

class OllamaAgentStreamingCallbackHandler(BaseCallbackHandler):
    def __init__(self, is_user_talking, user_input_queue, llm_output_queue, llm_output_queue_ws):
        self.is_user_talking = is_user_talking
        self.user_input_queue = user_input_queue
        self.llm_output_queue = llm_output_queue
        self.llm_output_queue_ws = llm_output_queue_ws
        self.llm_output = ""  # ç”¨äºç¼“å­˜åˆ†æ®µå“åº”ç„¶å¾Œè¼¸å…¥ tts
        self.full_response = ""  # ç”¨äºç¼“å­˜å®Œæ•´å“åº”
        # self.is_final_answer = False
        self.is_put_to_llm_output_queue = False
        self.token_window = deque(maxlen=9)  # æ»‘åŠ¨çª—å£
    
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        # # æµå¼è¾“å‡ºæ¯ä¸ª Tokenï¼ˆOllama çš„ token å¯èƒ½åŒ…å«æ ¼å¼å­—ç¬¦ï¼‰
        # self.full_response += token
        # print(f"\033[95m|\033[0m", end="", flush=True)  # ç´«è‰²é«˜äº®è¾“å‡º

        # # æ›´æ–°æ»‘åŠ¨çª—å£
        # self.token_window.append(token)
        # self.llm_output_queue_ws.put(token)
        # # æ£€æŸ¥æ»‘åŠ¨çª—å£ä¸­çš„ token æ˜¯å¦åŒ¹é… ' "Final Answer",\n'
        # if not self.is_final_answer and "".join(self.token_window) == 'Final Answer",\n  "action_input": "':
        #     self.is_final_answer = True
        #     self.is_put_to_llm_output_queue = True
        #     print("\n\033[91mğŸ¤– Action: Final Answer\033[0m") #

        # elif self.is_final_answer:
        #     if self.is_user_talking.is_set() or not self.user_input_queue.empty():
        #         if not self.llm_output_queue.empty():
        #             empty_queue = self.llm_output_queue.get(block=False)
        #         return
            
        #     # Directly append to llm_output, reducing queue operations
        #     self.llm_output += token

        #     if '"' in token:
        #         self.is_final_answer = False
        #         return

        #     if any(punct in token for punct in ["ï¼Œ", ",", "ã€‚", ".", "ï¼Ÿ", "?", "ï¼", "!"]):
        #         self.llm_output_queue.put(self.llm_output)
        #         self.llm_output = ""
            # self.neo4j.add_dialogue_record(user_message, llm_message)
        pass

# 2. å˜—è©¦ä¸ç”¨ langchain çš„æƒ…æ³ä¸‹é€šéæç¤ºè©å˜—è©¦è®“æ¨¡å‹ç”Ÿæˆ json æˆ–è€… code çš„ tools å‘¼å«
# 3. åœ¨æŠ±æŠ±è‡‰çš„ dc ç¾¤çµ„ä¸­è©¢å•æˆ‘å° langchain çš„ç†è§£æ˜¯ä¸æ˜¯æ­£ç¢ºçš„ï¼Œç¾åœ¨ langchain è¡¨ç¾ä¸å¥½æ˜¯å¦å› çˆ²æˆ‘å° langchain çš„ä½¿ç”¨éŒ¯èª¤
# 4. é€šéä¿®æ”¹æç¤ºè©æ¨¡æ¿å’Œ OllamaAgentStreamingCallbackHandler ä¾†è§£æ±º ollama é‹è¡Œçš„æ¨¡å‹æœ‰æ™‚å€™è¼¸å‡º </think> æœ‰æ™‚å€™æ²’æœ‰è¼¸å‡ºçš„å•é¡Œï¼Œé€™æœƒå°è‡´ç„¡æ³•å§æ­£ç¢ºçš„å†…å®¹è¼¸å‡ºåˆ° tts
5. æœç´¢çš„ä¿¡æ¯ä¸­å˜—è©¦åŒ…å«é£Ÿç‰©æˆ–è€…é£Ÿè­œçš„åœ–ç‰‡
6. åœ¨é£Ÿè­œå’Œé£Ÿç‰©çš„æ•¸æ“šé é¢æ·»åŠ ä¸€å€‹æŒ‰éˆ•ï¼Œç•¶ç”¨æˆ¶é»æ“Šæ™‚ï¼Œæœƒç”Ÿæˆç¸½çµ
7. MemGPT for llm memory


    def on_agent_action(self, action, **kwargs):
        # Agent è°ƒç”¨å·¥å…·æ—¶è§¦å‘
        # print(f"\n\033[94mğŸ¤– Action: {action.log}\033[0m")  # è“è‰²é«˜äº®
        # print(f"\n\033[91mğŸ¤– Action: {action.log}\033[0m")  # çº¢è‰²é«˜äº®
        # self.queue.put(f"\nAction: {action.log}")
        pass

    def on_tool_end(self, output: str, **kwargs):
        # å·¥å…·æ‰§è¡Œå®Œæˆ
        # print(f"\n\033[93mğŸ” Observation: {output}\033[0m")  # é»„è‰²é«˜äº®
        # print("on_tool_end called")
        # print(f"\n\033[38;5;208mğŸ” Observation: {output}\033[0m")  # æ©™è‰²é«˜äº® (256-color)
        # self.queue.put(f"\nObservation: {output}")
        pass

    def on_agent_finish(self, finish, **kwargs):
        # Agent å®Œæˆæ‰€æœ‰æ“ä½œ
        output = finish.return_values['output']
        print(f"\n\033[38;5;208mğŸ” return_values['output']: {output}\033[0m")  # æ©™è‰²é«˜äº® (256-color)
        llm_output = ""

        for words in output:
            if self.is_user_talking.is_set() or not self.user_input_queue.empty():
                if not self.llm_output_queue.empty():
                    empty_queue = self.llm_output_queue.get(block=False)
                break
            
            # Directly append to llm_output, reducing queue operations
            llm_output += words

            if words in ["ï¼Œ", ",", "ã€‚", ".", "ï¼Ÿ", "?", "ï¼", "!"] or "</think>" in words:
                
                if self.is_put_to_llm_output_queue:
                    self.llm_output_queue.put(llm_output)
                self.llm_output_queue_ws.put(llm_output)
                if "</think>" in llm_output: self.is_put_to_llm_output_queue = True
                if "<|IS|>" in llm_output: self.is_put_to_llm_output_queue = False

                # print("llm words: " + llm_output, "  self.llm_output_queue: " + str(self.llm_output_queue.qsize()))
                llm_output = ""

        # self.neo4j.add_dialogue_record(user_message, llm_message)
        pass

    def on_error(self, error, **kwargs):
        print(f"\nğŸ”¥ Error: {str(error)}")

class GoogleAgentStreamingCallbackHandler(BaseCallbackHandler):
    def __init__(self, is_user_talking, user_input_queue, llm_output_queue, llm_output_queue_ws):
        self.is_user_talking = is_user_talking
        self.user_input_queue = user_input_queue
        self.llm_output_queue = llm_output_queue
        self.llm_output_queue_ws = llm_output_queue_ws

    def on_agent_action(self, action, **kwargs):
        # Agent è°ƒç”¨å·¥å…·æ—¶è§¦å‘
        # print(f"\n\033[94mğŸ¤– Action: {action.log}\033[0m")  # è“è‰²é«˜äº®
        # print(f"\n\033[91mğŸ¤– Action: {action.log}\033[0m")  # çº¢è‰²é«˜äº®
        # self.llm_output_queue.put(f"\nAction: {action.log}")
        pass

    def on_agent_finish(self, finish, **kwargs):
        # Agent å®Œæˆæ‰€æœ‰æ“ä½œ
        output = finish.return_values['output']
        print(f"\n\033[38;5;208mğŸ” return_values['output']: {output}\033[0m")  # æ©™è‰²é«˜äº® (256-color)
        self.llm_output_queue_ws.put(output)
        llm_output = ""

        for words in output:
            if self.is_user_talking.is_set() or not self.user_input_queue.empty():
                if not self.llm_output_queue.empty():
                    empty_queue = self.llm_output_queue.get(block=False)
                break
            
            # Directly append to llm_output, reducing queue operations
            llm_output += words
            if "<|IS|>" in llm_output: break

            if words in ["ï¼Œ", ",", "ã€‚", ".", "ï¼Ÿ", "?", "ï¼", "!"]:
                self.llm_output_queue.put(llm_output)
                # print("llm words: " + llm_output, "  self.llm_output_queue: " + str(self.llm_output_queue.qsize()))
                llm_output = ""

        # self.neo4j.add_dialogue_record(user_message, llm_message)
        pass