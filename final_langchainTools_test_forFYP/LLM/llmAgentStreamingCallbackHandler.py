from langchain.callbacks.base import BaseCallbackHandler

class OllamaAgentStreamingCallbackHandler(BaseCallbackHandler):
    def __init__(self, is_user_talking, user_input_queue, llm_output_queue, llm_output_queue_ws):
        self.is_user_talking = is_user_talking
        self.user_input_queue = user_input_queue
        self.llm_output_queue = llm_output_queue
        self.llm_output_queue_ws = llm_output_queue_ws
        self.llm_output = ""  # ç”¨äºç¼“å­˜åˆ†æ®µå“åº”ç„¶å¾Œè¼¸å…¥ tts
        self.full_response = ""  # ç”¨äºç¼“å­˜å®Œæ•´å“åº”
        self.is_agent_action = False
        self.is_llm_thinking = False
        self.token_window = []  # æ»‘åŠ¨çª—å£
    
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        # æµå¼è¾“å‡ºæ¯ä¸ª Tokenï¼ˆOllama çš„ token å¯èƒ½åŒ…å«æ ¼å¼å­—ç¬¦ï¼‰
        self.full_response += token
        # print(f"\033[95m{token}\033[0m", end="", flush=True)  # ç´«è‰²é«˜äº®è¾“å‡º

        # æ›´æ–°æ»‘åŠ¨çª—å£
        self.token_window.append(token)
        if len(self.token_window) > 4:  # å‡è®¾ ' "Final Answer",\n' æ˜¯ç”± 4 ä¸ª token ç»„æˆ
            self.token_window.pop(0)

        # æ£€æŸ¥æ»‘åŠ¨çª—å£ä¸­çš„ token æ˜¯å¦åŒ¹é… ' "Final Answer",\n'
        print("----------------" + "".join(self.token_window) + "----------------")
        if "".join(self.token_window) == ' "Final Answer",\n':
            self.is_agent_action = True
            print("\n\033[91mğŸ¤– Action: Final Answer\033[0m") #

        if self.is_agent_action:
            if self.is_user_talking.is_set() or not self.user_input_queue.empty():
                if not self.llm_output_queue.empty():
                    empty_queue = self.llm_output_queue.get(block=False)
                return
            
            # Directly append to llm_output, reducing queue operations
            self.llm_output += token
            if "<think>" in token and not self.is_llm_thinking:
                self.is_llm_thinking = True
                print("self.is_llm_thinking = True")

            elif "</think>" in token and self.is_llm_thinking:
                self.is_llm_thinking = False
                print("self.is_llm_thinking = False")

            if token in ["ï¼Œ", ",", "ã€‚", ".", "ï¼Ÿ", "?", "ï¼", "!"] or "</think>" in token:
                print("\n\n   ---llm token: " + self.llm_output + "---\n\n")
                if not self.is_llm_thinking and "</think>" in token:
                    self.llm_output_queue.put(self.llm_output)
                self.llm_output_queue_ws.put(self.llm_output)
                llm_output = ""

            # self.neo4j.add_dialogue_record(user_message, llm_message)

    def on_agent_action(self, action, **kwargs):
        # Agent è°ƒç”¨å·¥å…·æ—¶è§¦å‘
        # print(f"\n\033[94mğŸ¤– Action: {action.log}\033[0m")  # è“è‰²é«˜äº®
        # print(f"\n\033[91mğŸ¤– Action: {action.log}\033[0m")  # çº¢è‰²é«˜äº®
        # self.queue.put(f"\nAction: {action.log}")
        pass

    def on_tool_end(self, output: str, **kwargs):
        # å·¥å…·æ‰§è¡Œå®Œæˆ
        # print(f"\n\033[93mğŸ” Observation: {output}\033[0m")  # é»„è‰²é«˜äº®
        # print("on_tool_end called")
        # print(f"\n\033[38;5;208mğŸ” Observation: {output}\033[0m")  # æ©™è‰²é«˜äº® (256-color)
        # self.queue.put(f"\nObservation: {output}")
        pass

    def on_agent_finish(self, finish, **kwargs):
        # Agent å®Œæˆæ‰€æœ‰æ“ä½œ
        # print(f"\n\033[95mâœ… Final Answer: {finish.return_values['output']}\033[0m")
        # self.queue.put(f"\nFinal Result: {finish.return_values['output']}")
        # self.queue.put(None)  # ç»“æŸä¿¡å·
        self.is_agent_action = False
        pass

class GoogleAgentStreamingCallbackHandler(BaseCallbackHandler):
    def __init__(self, is_user_talking, user_input_queue, llm_output_queue, llm_output_queue_ws):
        self.is_user_talking = is_user_talking
        self.user_input_queue = user_input_queue
        self.llm_output_queue = llm_output_queue
        self.llm_output_queue_ws = llm_output_queue_ws

    def on_agent_action(self, action, **kwargs):
        # Agent è°ƒç”¨å·¥å…·æ—¶è§¦å‘
        # print(f"\n\033[94mğŸ¤– Action: {action.log}\033[0m")  # è“è‰²é«˜äº®
        # print(f"\n\033[91mğŸ¤– Action: {action.log}\033[0m")  # çº¢è‰²é«˜äº®
        # self.llm_output_queue.put(f"\nAction: {action.log}")
        pass

    def on_agent_finish(self, finish, **kwargs):
        # Agent å®Œæˆæ‰€æœ‰æ“ä½œ
        output = finish.return_values['output']
        print(f"\n\033[38;5;208mğŸ” return_values['output']: {output}\033[0m")  # æ©™è‰²é«˜äº® (256-color)
        self.llm_output_queue_ws.put(output)
        llm_output = ""

        for words in output:
            if self.is_user_talking.is_set() or not self.user_input_queue.empty():
                if not self.llm_output_queue.empty():
                    empty_queue = self.llm_output_queue.get(block=False)
                break
            
            # Directly append to llm_output, reducing queue operations
            llm_output += words
            if "<|IS|>" in llm_output: break

            if words in ["ï¼Œ", ",", "ã€‚", ".", "ï¼Ÿ", "?", "ï¼", "!"]:
                self.llm_output_queue.put(llm_output)
                # print("llm words: " + llm_output, "  self.llm_output_queue: " + str(self.llm_output_queue.qsize()))
                llm_output = ""

        # self.neo4j.add_dialogue_record(user_message, llm_message)
        pass