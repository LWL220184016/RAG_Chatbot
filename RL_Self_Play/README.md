# 自我對弈強化學習系統

這個系統實現了一個AI模型與自己的分身進行對話來進行強化學習微調的框架。

## 系統架構

- `train.py`: 主訓練腳本，協調整個自我對弈和強化學習流程
- `agent.py`: DialogueAgent類，負責生成對話回應
- `reward_model.py`: RewardModel類，計算對話品質的獎勵分數
- `config.py`: 所有配置參數
- `test_self_play.py`: 測試腳本，驗證系統組件

## 工作原理

1. **自我對弈**: 同一個模型的兩個實例（Agent 0 和 Agent 1）輪流進行對話
2. **獎勵計算**: 每個回應都會通過獎勵模型評估品質
3. **PPO訓練**: 使用PPO算法根據獎勵信號調整模型參數
4. **時間感知**: 模型學習適當的對話時機和沉默

## 特性

- ✅ 支援GPU和CPU運行
- ✅ 可配置的模型和訓練參數
- ✅ 智能的獎勵系統（鼓勵有意義的對話，懲罰過於簡短或重複的回應）
- ✅ 時間感知對話（模型學習何時說話何時沉默）
- ✅ 對話歷史管理和記錄
- ✅ 定期模型儲存

## 安裝依賴

```bash
pip install torch transformers trl tqdm datasets
```

## 使用方法

### 1. 測試系統

首先運行測試腳本確保系統正常工作：

```bash
cd RL_Self_Play
python test_self_play.py
```

### 2. 配置參數

編輯 `config.py` 調整以下參數：

- `MODEL_ID`: 基礎語言模型
- `REWARD_MODEL_ID`: 獎勵模型
- `TOTAL_EPISODES`: 訓練回合數
- `MAX_TURNS`: 每回合最大對話輪次
- `PPO_CONFIG`: PPO訓練參數

### 3. 開始訓練

```bash
python train.py
```

## 配置說明

### 模型設定
- 默認使用 Qwen3-1.7B 作為基礎模型
- 使用 Skywork-Reward-V2 作為獎勵模型
- 支援任何 Hugging Face 模型

### 訓練設定
- PPO學習率: 1.41e-5
- 批次大小: 1（適合小型模型和有限GPU記憶體）
- 每回合對話輪次: 10（可調整）
- 總訓練回合: 5（可調整）

### 獎勵系統
- 鼓勵有意義的回應（長度 > 5字符）
- 鼓勵禮貌用語
- 懲罰過於簡短的回應
- 適當的沉默時機獎勵

## 輸出文件

- `ppo_self_play_model/`: 訓練好的模型儲存目錄
- 對話歷史記錄在記憶系統中

## 高級用法

### 自定義獎勵函數

編輯 `reward_model.py` 中的 `_simple_reward` 方法來實現自定義獎勵邏輯。

### 自定義對話主題

編輯 `train.py` 中的 `conversation_topics` 列表來添加更多對話主題。

### 調整生成參數

編輯 `config.py` 中的 `GENERATION_KWARGS` 來調整文本生成行為。

## 故障排除

1. **記憶體不足**: 減少 batch_size 或使用更小的模型
2. **CUDA錯誤**: 設定 `DEVICE = "cpu"` 在CPU上運行
3. **模型載入失敗**: 確保有網路連接下載模型

## 注意事項

- 首次運行會下載模型，需要良好的網路連接
- GPU記憶體需求約4-8GB（取決於模型大小）
- 訓練時間取決於硬體配置和參數設定

## 擴展建議

1. 添加更複雜的獎勵函數
2. 實現多代理對話（>2個代理）
3. 添加對話主題分類和專業化
4. 集成更好的評估指標
5. 實現增量學習和模型版本管理
