from vllm import LLM, SamplingParams

llm = LLM(
    # model="deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    model="deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    quantization="fp8",  # å¯é€‰: "awq" æˆ– "gptq"
    dtype="float16",  # æˆ–è€…ä½¿ç”¨ "bfloat16"
    gpu_memory_utilization=0.98,  # å¯é€‰: æ§åˆ¶ GPU å†…å­˜ä½¿ç”¨ç‡
    max_model_len=20480, 
)

# å¯é€‰: å®šä¹‰é‡‡æ ·å‚æ•°
sampling_params = SamplingParams( 
    temperature=0.7, 
    top_p=0.95, 
    max_tokens=1024, 
) 

while True:
    user_input = input("User: ")
    if user_input == "exit":
        break
    else:
        output = llm.generate(user_input, sampling_params=sampling_params)
        response = output[0].outputs[0].text
        print(f"\n\033[38;5;208mğŸ” LLM: {response}\033[0m")  # æ©™è‰²é«˜äº® (256-color)