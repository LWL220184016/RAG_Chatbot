from langchain.callbacks.base import BaseCallbackHandler

class LLMAgentStreamingCallbackHandler(BaseCallbackHandler):
    def ___init__(self):
        self.full_response = ""  # ç”¨äºç¼“å­˜å®Œæ•´å“åº”
    
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        # æµå¼è¾“å‡ºæ¯ä¸ª Tokenï¼ˆOllama çš„ token å¯èƒ½åŒ…å«æ ¼å¼å­—ç¬¦ï¼‰
        self.full_response += token
        print(f"\033[92m{token}\033[0m", end="", flush=True)  # ç»¿è‰²é«˜äº®è¾“å‡º

    def on_agent_action(self, action, **kwargs):
        # Agent è°ƒç”¨å·¥å…·æ—¶è§¦å‘
        print(f"\n\033[94mğŸ¤– Action: {action.log}\033[0m")  # è“è‰²é«˜äº®

    def on_tool_end(self, output: str, **kwargs):
        # å·¥å…·æ‰§è¡Œå®Œæˆ
        print(f"\n\033[93mğŸ” Observation: {output}\033[0m")  # é»„è‰²é«˜äº®

    def on_agent_finish(self, finish, **kwargs):
        # Agent å®Œæˆæ‰€æœ‰æ“ä½œ
        print(f"\n\033[92mâœ… Final Answer: {finish.return_values['output']}\033[0m")